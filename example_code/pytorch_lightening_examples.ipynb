{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: From PyTorch to PyTorch Lightning ðŸš€\n",
    "\n",
    "This notebook provides a detailed walkthrough on how to convert a standard PyTorch project to PyTorch Lightning, using the **Fashion MNIST** dataset.\n",
    "\n",
    "## Introduction to PyTorch Lightning\n",
    "\n",
    "PyTorch is a fantastic and flexible library for building deep learning models. However, when projects grow, you often find yourself writing the same boilerplate code again and againâ€”training loops, validation loops, device management (`.to(device)`), etc.\n",
    "\n",
    "**PyTorch Lightning** is a lightweight wrapper on top of PyTorch that organizes your code and automates the engineering and training loops. This lets you focus on the research and modeling parts.\n",
    "\n",
    "**Key Benefits:**\n",
    "* **Organized Code:** Your code is neatly structured into a `LightningModule` (the model and logic) and a `LightningDataModule` (the data handling).\n",
    "* **Boilerplate Free:** Lightning handles the training, validation, and testing loops for you. No more manual `for` loops!\n",
    "* **Hardware Agnostic:** The same code runs on CPUs, GPUs, or TPUs with a simple flag change (e.g., `accelerator='gpu', devices=4`).\n",
    "* **Reproducibility:** Lightning helps make your experiments more reproducible.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1.  Build and train a simple CNN on the **Fashion MNIST** dataset using **standard PyTorch**.\n",
    "2.  Refactor the same project using **PyTorch Lightning**, highlighting the changes and benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 1: The Standard PyTorch Way\n",
    "\n",
    "First, let's build and train our model using only plain PyTorch. This will be our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # mps for apple silicon\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Model\n",
    "\n",
    "Here we define a simple Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Fully connected layers\n",
    "        # The input features are calculated as 64 (channels) * 7 * 7 (image size after 2 pooling layers)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10) # 10 output classes for Fashion MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply conv1 -> ReLU -> pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply conv2 -> ReLU -> pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the image tensor for the fully connected layers\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # Apply fc1 -> ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply output layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data_full = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Split the full training data into a smaller training set and a validation set\n",
    "train_size = int(0.8 * len(training_data_full))\n",
    "val_size = len(training_data_full) - train_size\n",
    "training_data, val_data = random_split(training_data_full, [train_size, val_size])\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 The Training Loop (The Boilerplate!)\n",
    "This is the core part that PyTorch Lightning will automate for us. Notice how much manual work is involved: moving data to the device, zeroing gradients, backpropagation, and manually looping through epochs and batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate Model, Loss, and Optimizer ---\n",
    "model = SimpleCNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- Training and Validation Loop ---\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- Training ---\n",
    "    model.train() # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # Move data to the selected device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 2. Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # No need to calculate gradients during validation\n",
    "        for X, y in val_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {epoch_secs:.2f}s')\n",
    "    print(f'\\tTrain Loss: {avg_train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {avg_val_loss:.3f} |  Val. Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works perfectly fine, but it's a lot of code to manage. If you wanted to add GPU training, logging, or gradient clipping, this section would get even more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 2: The PyTorch Lightning Way âœ¨\n",
    "\n",
    "Now, let's refactor the code above using PyTorch Lightning. We will separate the data logic from the model logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Imports and Setup\n",
    "\n",
    "We add imports for `pytorch_lightning` and `torchmetrics`. `torchmetrics` is a library that integrates seamlessly with Lightning for calculating metrics efficiently.\n",
    "\n",
    "First, let's install them if they aren't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning torchmetrics --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NEW: Import PyTorch Lightning and TorchMetrics\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "# For reproducibility\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The `LightningDataModule` (Organizing Data)\n",
    "A `LightningDataModule` is a shareable, reusable class that encapsulates all the steps needed to process data.\n",
    "\n",
    "* `prepare_data()`: This is for downloading and saving data to disk. It's called on **only one GPU/process** to avoid downloading the data multiple times.\n",
    "* `setup()`: This is for splitting, transforming, and creating the PyTorch `Dataset` objects. It's called on **every GPU/process**.\n",
    "* `..._dataloader()`: These methods return the actual `DataLoader` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./data\", batch_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "    # This is called on one process only.\n",
    "    # Use it for downloading the dataset.\n",
    "    def prepare_data(self):\n",
    "        datasets.FashionMNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.FashionMNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    # This is called on every process (GPU).\n",
    "    # Use it to split data, create datasets, etc.\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Load the full training dataset\n",
    "            fashion_mnist_full = datasets.FashionMNIST(\n",
    "                self.data_dir, train=True, transform=self.transform\n",
    "            )\n",
    "            # Split it into training and validation sets\n",
    "            self.fashion_mnist_train, self.fashion_mnist_val = random_split(\n",
    "                fashion_mnist_full, [50000, 10000]\n",
    "            )\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.fashion_mnist_test = datasets.FashionMNIST(\n",
    "                self.data_dir, train=False, transform=self.transform\n",
    "            )\n",
    "\n",
    "    # Return the training dataloader\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.fashion_mnist_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    # Return the validation dataloader\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.fashion_mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    # Return the test dataloader\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.fashion_mnist_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The `LightningModule` (Organizing the Model & Logic)\n",
    "\n",
    "The `LightningModule` is where the magic happens. We take our `nn.Module` and add hooks for training and validation logic. It combines the model architecture, the optimizer, and the training/validation steps into one clean class.\n",
    "\n",
    "* `__init__()`: Define your model layers, loss function, and metrics here. We use `torchmetrics.Accuracy`.\n",
    "* `forward()`: The standard forward pass, just like in `nn.Module`.\n",
    "* `training_step()`: This replaces the inner training loop. It receives a batch, performs a forward pass, and returns the loss. Lightning handles the rest (backpropagation, optimizer step, etc.).\n",
    "* `validation_step()`: Replaces the inner validation loop. You log your metrics here using `self.log()`.\n",
    "* `configure_optimizers()`: You define your optimizer here and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitFashionMNIST(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters like learning_rate\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # --- Model Architecture (same as before) ---\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        # --- Loss Function ---\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # --- Metrics ---\n",
    "        # We use TorchMetrics to calculate accuracy.\n",
    "        # 'task' is set to 'multiclass' and we specify the number of classes.\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    # The forward pass for inference\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    # This is the training loop.\n",
    "    # It is called for each batch in the training dataloader.\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        pred = self(X)  # Same as self.forward(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "\n",
    "        # Use self.log to record the training loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss # This is required\n",
    "\n",
    "    # This is the validation loop.\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        acc = self.accuracy(pred, y)\n",
    "\n",
    "        # Use self.log to record validation loss and accuracy\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    # Define the optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 The `Trainer` (Running the Show)\n",
    "\n",
    "Finally, the `pl.Trainer` object takes your `LightningModule` and `LightningDataModule` and handles the entire training process.\n",
    "\n",
    "You just need to tell it how many epochs to run and what hardware to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate DataModule and Model ---\n",
    "datamodule = FashionMNISTDataModule()\n",
    "model = LitFashionMNIST()\n",
    "\n",
    "# --- Instantiate the Trainer ---\n",
    "# The Trainer automates the training, validation, and testing loops.\n",
    "# You can specify the number of epochs, accelerator (cpu, gpu, tpu), devices, etc.\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\", # Automatically selects GPU if available\n",
    "    devices=\"auto\"\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "# The .fit() method takes the model and the datamodule.\n",
    "# It will automatically call the setup, dataloaders, and training/validation steps.\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! With just these few lines, you get a full training loop with progress bars, logging, and hardware acceleration, all handled for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Comparison and Conclusion\n",
    "\n",
    "| Task                 | Standard PyTorch                          | PyTorch Lightning                                                 |\n",
    "| :------------------- | :---------------------------------------- | :---------------------------------------------------------------- |\n",
    "| **Model Definition** | `nn.Module` class                         | `pl.LightningModule` class                                        |\n",
    "| **Data Handling** | Manual `Dataset` and `DataLoader` creation | `pl.LightningDataModule` class                                    |\n",
    "| **Training Loop** | Manual `for` loop over epochs and batches   | Handled by `Trainer.fit()` and `training_step`                      |\n",
    "| **Validation Loop** | Manual `for` loop with `torch.no_grad()`    | Handled by `Trainer.fit()` and `validation_step`                    |\n",
    "| **Optimizer Logic** | Manual `.zero_grad()`, `.backward()`, `.step()` | Handled by `Trainer`; you just define it in `configure_optimizers`  |\n",
    "| **Device Management**| Manual `.to(device)` calls                | Handled automatically by the `Trainer` (`accelerator` flag)         |\n",
    "| **Metrics** | Manual calculation                        | Integrated via `torchmetrics` and `self.log()`                      |\n",
    "\n",
    "By refactoring to PyTorch Lightning, you've made your code cleaner, more organized, and ready to scale to more complex models and hardware without changing the core logic. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
